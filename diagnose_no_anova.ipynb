{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('F:\\leukemi_microarray\\Leukemia_GSE9476.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df = df.drop(columns=['samples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1053_at</th>\n",
       "      <th>117_at</th>\n",
       "      <th>121_at</th>\n",
       "      <th>1255_g_at</th>\n",
       "      <th>1294_at</th>\n",
       "      <th>1316_at</th>\n",
       "      <th>1320_at</th>\n",
       "      <th>1405_i_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-r2-Hs28SrRNA-5_at</th>\n",
       "      <th>AFFX-r2-Hs28SrRNA-M_at</th>\n",
       "      <th>AFFX-r2-P1-cre-3_at</th>\n",
       "      <th>AFFX-r2-P1-cre-5_at</th>\n",
       "      <th>AFFX-ThrX-3_at</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bone_Marrow</td>\n",
       "      <td>7.813454</td>\n",
       "      <td>6.794223</td>\n",
       "      <td>6.576484</td>\n",
       "      <td>8.904530</td>\n",
       "      <td>4.506871</td>\n",
       "      <td>7.893402</td>\n",
       "      <td>5.532314</td>\n",
       "      <td>4.558692</td>\n",
       "      <td>8.601630</td>\n",
       "      <td>...</td>\n",
       "      <td>4.620766</td>\n",
       "      <td>6.573429</td>\n",
       "      <td>12.562943</td>\n",
       "      <td>12.435309</td>\n",
       "      <td>5.339725</td>\n",
       "      <td>4.535060</td>\n",
       "      <td>3.864028</td>\n",
       "      <td>3.576569</td>\n",
       "      <td>4.081583</td>\n",
       "      <td>3.985984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AML</td>\n",
       "      <td>7.662544</td>\n",
       "      <td>7.352310</td>\n",
       "      <td>7.607587</td>\n",
       "      <td>8.694334</td>\n",
       "      <td>4.386515</td>\n",
       "      <td>7.663462</td>\n",
       "      <td>5.555711</td>\n",
       "      <td>4.658772</td>\n",
       "      <td>5.430714</td>\n",
       "      <td>...</td>\n",
       "      <td>4.633553</td>\n",
       "      <td>6.085707</td>\n",
       "      <td>12.580625</td>\n",
       "      <td>12.557963</td>\n",
       "      <td>5.212672</td>\n",
       "      <td>4.553464</td>\n",
       "      <td>3.855921</td>\n",
       "      <td>3.655604</td>\n",
       "      <td>4.139994</td>\n",
       "      <td>4.046524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bone_Marrow</td>\n",
       "      <td>7.892896</td>\n",
       "      <td>6.538820</td>\n",
       "      <td>6.673375</td>\n",
       "      <td>8.887987</td>\n",
       "      <td>4.359906</td>\n",
       "      <td>7.972636</td>\n",
       "      <td>5.473035</td>\n",
       "      <td>4.785807</td>\n",
       "      <td>8.389563</td>\n",
       "      <td>...</td>\n",
       "      <td>4.880272</td>\n",
       "      <td>6.483972</td>\n",
       "      <td>12.994929</td>\n",
       "      <td>13.036519</td>\n",
       "      <td>5.231631</td>\n",
       "      <td>4.602278</td>\n",
       "      <td>3.814854</td>\n",
       "      <td>3.581013</td>\n",
       "      <td>4.080418</td>\n",
       "      <td>4.073512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bone_Marrow</td>\n",
       "      <td>7.720314</td>\n",
       "      <td>6.583612</td>\n",
       "      <td>6.756619</td>\n",
       "      <td>8.845196</td>\n",
       "      <td>4.477831</td>\n",
       "      <td>8.064107</td>\n",
       "      <td>5.539644</td>\n",
       "      <td>4.537853</td>\n",
       "      <td>7.226976</td>\n",
       "      <td>...</td>\n",
       "      <td>5.191044</td>\n",
       "      <td>6.824745</td>\n",
       "      <td>13.123380</td>\n",
       "      <td>13.057701</td>\n",
       "      <td>5.230581</td>\n",
       "      <td>4.416719</td>\n",
       "      <td>3.890692</td>\n",
       "      <td>3.551122</td>\n",
       "      <td>4.097900</td>\n",
       "      <td>3.976198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>PB</td>\n",
       "      <td>8.104750</td>\n",
       "      <td>6.212916</td>\n",
       "      <td>9.229483</td>\n",
       "      <td>8.968546</td>\n",
       "      <td>4.566027</td>\n",
       "      <td>7.722725</td>\n",
       "      <td>5.661172</td>\n",
       "      <td>4.832844</td>\n",
       "      <td>7.742639</td>\n",
       "      <td>...</td>\n",
       "      <td>4.313892</td>\n",
       "      <td>6.209274</td>\n",
       "      <td>13.256962</td>\n",
       "      <td>13.003892</td>\n",
       "      <td>5.290075</td>\n",
       "      <td>4.514481</td>\n",
       "      <td>3.946614</td>\n",
       "      <td>3.602132</td>\n",
       "      <td>4.206121</td>\n",
       "      <td>4.103831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bone_Marrow</td>\n",
       "      <td>7.966060</td>\n",
       "      <td>6.781246</td>\n",
       "      <td>6.920990</td>\n",
       "      <td>8.978862</td>\n",
       "      <td>4.478028</td>\n",
       "      <td>7.683032</td>\n",
       "      <td>5.610469</td>\n",
       "      <td>4.824193</td>\n",
       "      <td>7.502364</td>\n",
       "      <td>...</td>\n",
       "      <td>4.742075</td>\n",
       "      <td>6.794769</td>\n",
       "      <td>13.524380</td>\n",
       "      <td>13.400021</td>\n",
       "      <td>5.327728</td>\n",
       "      <td>4.631000</td>\n",
       "      <td>3.897626</td>\n",
       "      <td>3.671963</td>\n",
       "      <td>4.002604</td>\n",
       "      <td>4.058350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bone_Marrow</td>\n",
       "      <td>7.692476</td>\n",
       "      <td>6.775148</td>\n",
       "      <td>6.616971</td>\n",
       "      <td>8.706216</td>\n",
       "      <td>4.443770</td>\n",
       "      <td>7.692134</td>\n",
       "      <td>5.501473</td>\n",
       "      <td>4.558013</td>\n",
       "      <td>9.985429</td>\n",
       "      <td>...</td>\n",
       "      <td>5.082368</td>\n",
       "      <td>6.913332</td>\n",
       "      <td>12.511607</td>\n",
       "      <td>12.470305</td>\n",
       "      <td>5.224274</td>\n",
       "      <td>4.517029</td>\n",
       "      <td>3.844456</td>\n",
       "      <td>3.587143</td>\n",
       "      <td>4.119111</td>\n",
       "      <td>3.985250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AML</td>\n",
       "      <td>7.833074</td>\n",
       "      <td>6.444448</td>\n",
       "      <td>6.422178</td>\n",
       "      <td>8.866566</td>\n",
       "      <td>4.642514</td>\n",
       "      <td>8.171570</td>\n",
       "      <td>5.619711</td>\n",
       "      <td>4.664392</td>\n",
       "      <td>5.976281</td>\n",
       "      <td>...</td>\n",
       "      <td>4.908617</td>\n",
       "      <td>6.919148</td>\n",
       "      <td>12.889525</td>\n",
       "      <td>12.586034</td>\n",
       "      <td>5.577722</td>\n",
       "      <td>4.555417</td>\n",
       "      <td>3.939002</td>\n",
       "      <td>3.558659</td>\n",
       "      <td>4.133050</td>\n",
       "      <td>4.331262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>AML</td>\n",
       "      <td>7.765633</td>\n",
       "      <td>6.561750</td>\n",
       "      <td>6.733191</td>\n",
       "      <td>8.864177</td>\n",
       "      <td>4.568688</td>\n",
       "      <td>8.329480</td>\n",
       "      <td>5.500327</td>\n",
       "      <td>4.674511</td>\n",
       "      <td>5.790804</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000191</td>\n",
       "      <td>6.646477</td>\n",
       "      <td>12.707219</td>\n",
       "      <td>12.559549</td>\n",
       "      <td>5.439620</td>\n",
       "      <td>4.661590</td>\n",
       "      <td>3.933793</td>\n",
       "      <td>3.686490</td>\n",
       "      <td>4.116434</td>\n",
       "      <td>4.041620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bone_Marrow_CD34</td>\n",
       "      <td>8.010117</td>\n",
       "      <td>7.405281</td>\n",
       "      <td>6.656049</td>\n",
       "      <td>9.050682</td>\n",
       "      <td>4.514986</td>\n",
       "      <td>8.377046</td>\n",
       "      <td>5.493713</td>\n",
       "      <td>4.860754</td>\n",
       "      <td>5.245049</td>\n",
       "      <td>...</td>\n",
       "      <td>5.305192</td>\n",
       "      <td>6.700453</td>\n",
       "      <td>12.949352</td>\n",
       "      <td>12.782515</td>\n",
       "      <td>5.341689</td>\n",
       "      <td>4.560315</td>\n",
       "      <td>3.887020</td>\n",
       "      <td>3.629853</td>\n",
       "      <td>4.127513</td>\n",
       "      <td>4.004316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                type  1007_s_at   1053_at    117_at    121_at  1255_g_at  \\\n",
       "17       Bone_Marrow   7.813454  6.794223  6.576484  8.904530   4.506871   \n",
       "25               AML   7.662544  7.352310  7.607587  8.694334   4.386515   \n",
       "14       Bone_Marrow   7.892896  6.538820  6.673375  8.887987   4.359906   \n",
       "13       Bone_Marrow   7.720314  6.583612  6.756619  8.845196   4.477831   \n",
       "48                PB   8.104750  6.212916  9.229483  8.968546   4.566027   \n",
       "8        Bone_Marrow   7.966060  6.781246  6.920990  8.978862   4.478028   \n",
       "9        Bone_Marrow   7.692476  6.775148  6.616971  8.706216   4.443770   \n",
       "31               AML   7.833074  6.444448  6.422178  8.866566   4.642514   \n",
       "32               AML   7.765633  6.561750  6.733191  8.864177   4.568688   \n",
       "4   Bone_Marrow_CD34   8.010117  7.405281  6.656049  9.050682   4.514986   \n",
       "\n",
       "     1294_at   1316_at   1320_at  1405_i_at  ...  AFFX-r2-Hs28SrRNA-5_at  \\\n",
       "17  7.893402  5.532314  4.558692   8.601630  ...                4.620766   \n",
       "25  7.663462  5.555711  4.658772   5.430714  ...                4.633553   \n",
       "14  7.972636  5.473035  4.785807   8.389563  ...                4.880272   \n",
       "13  8.064107  5.539644  4.537853   7.226976  ...                5.191044   \n",
       "48  7.722725  5.661172  4.832844   7.742639  ...                4.313892   \n",
       "8   7.683032  5.610469  4.824193   7.502364  ...                4.742075   \n",
       "9   7.692134  5.501473  4.558013   9.985429  ...                5.082368   \n",
       "31  8.171570  5.619711  4.664392   5.976281  ...                4.908617   \n",
       "32  8.329480  5.500327  4.674511   5.790804  ...                5.000191   \n",
       "4   8.377046  5.493713  4.860754   5.245049  ...                5.305192   \n",
       "\n",
       "    AFFX-r2-Hs28SrRNA-M_at  AFFX-r2-P1-cre-3_at  AFFX-r2-P1-cre-5_at  \\\n",
       "17                6.573429            12.562943            12.435309   \n",
       "25                6.085707            12.580625            12.557963   \n",
       "14                6.483972            12.994929            13.036519   \n",
       "13                6.824745            13.123380            13.057701   \n",
       "48                6.209274            13.256962            13.003892   \n",
       "8                 6.794769            13.524380            13.400021   \n",
       "9                 6.913332            12.511607            12.470305   \n",
       "31                6.919148            12.889525            12.586034   \n",
       "32                6.646477            12.707219            12.559549   \n",
       "4                 6.700453            12.949352            12.782515   \n",
       "\n",
       "    AFFX-ThrX-3_at  AFFX-ThrX-5_at  AFFX-ThrX-M_at  AFFX-TrpnX-3_at  \\\n",
       "17        5.339725        4.535060        3.864028         3.576569   \n",
       "25        5.212672        4.553464        3.855921         3.655604   \n",
       "14        5.231631        4.602278        3.814854         3.581013   \n",
       "13        5.230581        4.416719        3.890692         3.551122   \n",
       "48        5.290075        4.514481        3.946614         3.602132   \n",
       "8         5.327728        4.631000        3.897626         3.671963   \n",
       "9         5.224274        4.517029        3.844456         3.587143   \n",
       "31        5.577722        4.555417        3.939002         3.558659   \n",
       "32        5.439620        4.661590        3.933793         3.686490   \n",
       "4         5.341689        4.560315        3.887020         3.629853   \n",
       "\n",
       "    AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  \n",
       "17         4.081583         3.985984  \n",
       "25         4.139994         4.046524  \n",
       "14         4.080418         4.073512  \n",
       "13         4.097900         3.976198  \n",
       "48         4.206121         4.103831  \n",
       "8          4.002604         4.058350  \n",
       "9          4.119111         3.985250  \n",
       "31         4.133050         4.331262  \n",
       "32         4.116434         4.041620  \n",
       "4          4.127513         4.004316  \n",
       "\n",
       "[10 rows x 22284 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = shuffle(df)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "AML                 26\n",
      "Bone_Marrow         10\n",
      "PB                  10\n",
      "PBSC_CD34           10\n",
      "Bone_Marrow_CD34     8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "labels_count = df['type'].value_counts()\n",
    "print(labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1053_at</th>\n",
       "      <th>117_at</th>\n",
       "      <th>121_at</th>\n",
       "      <th>1255_g_at</th>\n",
       "      <th>1294_at</th>\n",
       "      <th>1316_at</th>\n",
       "      <th>1320_at</th>\n",
       "      <th>1405_i_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-r2-Hs28SrRNA-5_at</th>\n",
       "      <th>AFFX-r2-Hs28SrRNA-M_at</th>\n",
       "      <th>AFFX-r2-P1-cre-3_at</th>\n",
       "      <th>AFFX-r2-P1-cre-5_at</th>\n",
       "      <th>AFFX-ThrX-3_at</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>7.813454</td>\n",
       "      <td>6.794223</td>\n",
       "      <td>6.576484</td>\n",
       "      <td>8.904530</td>\n",
       "      <td>4.506871</td>\n",
       "      <td>7.893402</td>\n",
       "      <td>5.532314</td>\n",
       "      <td>4.558692</td>\n",
       "      <td>8.601630</td>\n",
       "      <td>...</td>\n",
       "      <td>4.620766</td>\n",
       "      <td>6.573429</td>\n",
       "      <td>12.562943</td>\n",
       "      <td>12.435309</td>\n",
       "      <td>5.339725</td>\n",
       "      <td>4.535060</td>\n",
       "      <td>3.864028</td>\n",
       "      <td>3.576569</td>\n",
       "      <td>4.081583</td>\n",
       "      <td>3.985984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>7.662544</td>\n",
       "      <td>7.352310</td>\n",
       "      <td>7.607587</td>\n",
       "      <td>8.694334</td>\n",
       "      <td>4.386515</td>\n",
       "      <td>7.663462</td>\n",
       "      <td>5.555711</td>\n",
       "      <td>4.658772</td>\n",
       "      <td>5.430714</td>\n",
       "      <td>...</td>\n",
       "      <td>4.633553</td>\n",
       "      <td>6.085707</td>\n",
       "      <td>12.580625</td>\n",
       "      <td>12.557963</td>\n",
       "      <td>5.212672</td>\n",
       "      <td>4.553464</td>\n",
       "      <td>3.855921</td>\n",
       "      <td>3.655604</td>\n",
       "      <td>4.139994</td>\n",
       "      <td>4.046524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>7.892896</td>\n",
       "      <td>6.538820</td>\n",
       "      <td>6.673375</td>\n",
       "      <td>8.887987</td>\n",
       "      <td>4.359906</td>\n",
       "      <td>7.972636</td>\n",
       "      <td>5.473035</td>\n",
       "      <td>4.785807</td>\n",
       "      <td>8.389563</td>\n",
       "      <td>...</td>\n",
       "      <td>4.880272</td>\n",
       "      <td>6.483972</td>\n",
       "      <td>12.994929</td>\n",
       "      <td>13.036519</td>\n",
       "      <td>5.231631</td>\n",
       "      <td>4.602278</td>\n",
       "      <td>3.814854</td>\n",
       "      <td>3.581013</td>\n",
       "      <td>4.080418</td>\n",
       "      <td>4.073512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>7.720314</td>\n",
       "      <td>6.583612</td>\n",
       "      <td>6.756619</td>\n",
       "      <td>8.845196</td>\n",
       "      <td>4.477831</td>\n",
       "      <td>8.064107</td>\n",
       "      <td>5.539644</td>\n",
       "      <td>4.537853</td>\n",
       "      <td>7.226976</td>\n",
       "      <td>...</td>\n",
       "      <td>5.191044</td>\n",
       "      <td>6.824745</td>\n",
       "      <td>13.123380</td>\n",
       "      <td>13.057701</td>\n",
       "      <td>5.230581</td>\n",
       "      <td>4.416719</td>\n",
       "      <td>3.890692</td>\n",
       "      <td>3.551122</td>\n",
       "      <td>4.097900</td>\n",
       "      <td>3.976198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>8.104750</td>\n",
       "      <td>6.212916</td>\n",
       "      <td>9.229483</td>\n",
       "      <td>8.968546</td>\n",
       "      <td>4.566027</td>\n",
       "      <td>7.722725</td>\n",
       "      <td>5.661172</td>\n",
       "      <td>4.832844</td>\n",
       "      <td>7.742639</td>\n",
       "      <td>...</td>\n",
       "      <td>4.313892</td>\n",
       "      <td>6.209274</td>\n",
       "      <td>13.256962</td>\n",
       "      <td>13.003892</td>\n",
       "      <td>5.290075</td>\n",
       "      <td>4.514481</td>\n",
       "      <td>3.946614</td>\n",
       "      <td>3.602132</td>\n",
       "      <td>4.206121</td>\n",
       "      <td>4.103831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>7.966060</td>\n",
       "      <td>6.781246</td>\n",
       "      <td>6.920990</td>\n",
       "      <td>8.978862</td>\n",
       "      <td>4.478028</td>\n",
       "      <td>7.683032</td>\n",
       "      <td>5.610469</td>\n",
       "      <td>4.824193</td>\n",
       "      <td>7.502364</td>\n",
       "      <td>...</td>\n",
       "      <td>4.742075</td>\n",
       "      <td>6.794769</td>\n",
       "      <td>13.524380</td>\n",
       "      <td>13.400021</td>\n",
       "      <td>5.327728</td>\n",
       "      <td>4.631000</td>\n",
       "      <td>3.897626</td>\n",
       "      <td>3.671963</td>\n",
       "      <td>4.002604</td>\n",
       "      <td>4.058350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>7.692476</td>\n",
       "      <td>6.775148</td>\n",
       "      <td>6.616971</td>\n",
       "      <td>8.706216</td>\n",
       "      <td>4.443770</td>\n",
       "      <td>7.692134</td>\n",
       "      <td>5.501473</td>\n",
       "      <td>4.558013</td>\n",
       "      <td>9.985429</td>\n",
       "      <td>...</td>\n",
       "      <td>5.082368</td>\n",
       "      <td>6.913332</td>\n",
       "      <td>12.511607</td>\n",
       "      <td>12.470305</td>\n",
       "      <td>5.224274</td>\n",
       "      <td>4.517029</td>\n",
       "      <td>3.844456</td>\n",
       "      <td>3.587143</td>\n",
       "      <td>4.119111</td>\n",
       "      <td>3.985250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>7.833074</td>\n",
       "      <td>6.444448</td>\n",
       "      <td>6.422178</td>\n",
       "      <td>8.866566</td>\n",
       "      <td>4.642514</td>\n",
       "      <td>8.171570</td>\n",
       "      <td>5.619711</td>\n",
       "      <td>4.664392</td>\n",
       "      <td>5.976281</td>\n",
       "      <td>...</td>\n",
       "      <td>4.908617</td>\n",
       "      <td>6.919148</td>\n",
       "      <td>12.889525</td>\n",
       "      <td>12.586034</td>\n",
       "      <td>5.577722</td>\n",
       "      <td>4.555417</td>\n",
       "      <td>3.939002</td>\n",
       "      <td>3.558659</td>\n",
       "      <td>4.133050</td>\n",
       "      <td>4.331262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>7.765633</td>\n",
       "      <td>6.561750</td>\n",
       "      <td>6.733191</td>\n",
       "      <td>8.864177</td>\n",
       "      <td>4.568688</td>\n",
       "      <td>8.329480</td>\n",
       "      <td>5.500327</td>\n",
       "      <td>4.674511</td>\n",
       "      <td>5.790804</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000191</td>\n",
       "      <td>6.646477</td>\n",
       "      <td>12.707219</td>\n",
       "      <td>12.559549</td>\n",
       "      <td>5.439620</td>\n",
       "      <td>4.661590</td>\n",
       "      <td>3.933793</td>\n",
       "      <td>3.686490</td>\n",
       "      <td>4.116434</td>\n",
       "      <td>4.041620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>8.010117</td>\n",
       "      <td>7.405281</td>\n",
       "      <td>6.656049</td>\n",
       "      <td>9.050682</td>\n",
       "      <td>4.514986</td>\n",
       "      <td>8.377046</td>\n",
       "      <td>5.493713</td>\n",
       "      <td>4.860754</td>\n",
       "      <td>5.245049</td>\n",
       "      <td>...</td>\n",
       "      <td>5.305192</td>\n",
       "      <td>6.700453</td>\n",
       "      <td>12.949352</td>\n",
       "      <td>12.782515</td>\n",
       "      <td>5.341689</td>\n",
       "      <td>4.560315</td>\n",
       "      <td>3.887020</td>\n",
       "      <td>3.629853</td>\n",
       "      <td>4.127513</td>\n",
       "      <td>4.004316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  1007_s_at   1053_at    117_at    121_at  1255_g_at   1294_at  \\\n",
       "17    1   7.813454  6.794223  6.576484  8.904530   4.506871  7.893402   \n",
       "25    0   7.662544  7.352310  7.607587  8.694334   4.386515  7.663462   \n",
       "14    1   7.892896  6.538820  6.673375  8.887987   4.359906  7.972636   \n",
       "13    1   7.720314  6.583612  6.756619  8.845196   4.477831  8.064107   \n",
       "48    3   8.104750  6.212916  9.229483  8.968546   4.566027  7.722725   \n",
       "8     1   7.966060  6.781246  6.920990  8.978862   4.478028  7.683032   \n",
       "9     1   7.692476  6.775148  6.616971  8.706216   4.443770  7.692134   \n",
       "31    0   7.833074  6.444448  6.422178  8.866566   4.642514  8.171570   \n",
       "32    0   7.765633  6.561750  6.733191  8.864177   4.568688  8.329480   \n",
       "4     2   8.010117  7.405281  6.656049  9.050682   4.514986  8.377046   \n",
       "\n",
       "     1316_at   1320_at  1405_i_at  ...  AFFX-r2-Hs28SrRNA-5_at  \\\n",
       "17  5.532314  4.558692   8.601630  ...                4.620766   \n",
       "25  5.555711  4.658772   5.430714  ...                4.633553   \n",
       "14  5.473035  4.785807   8.389563  ...                4.880272   \n",
       "13  5.539644  4.537853   7.226976  ...                5.191044   \n",
       "48  5.661172  4.832844   7.742639  ...                4.313892   \n",
       "8   5.610469  4.824193   7.502364  ...                4.742075   \n",
       "9   5.501473  4.558013   9.985429  ...                5.082368   \n",
       "31  5.619711  4.664392   5.976281  ...                4.908617   \n",
       "32  5.500327  4.674511   5.790804  ...                5.000191   \n",
       "4   5.493713  4.860754   5.245049  ...                5.305192   \n",
       "\n",
       "    AFFX-r2-Hs28SrRNA-M_at  AFFX-r2-P1-cre-3_at  AFFX-r2-P1-cre-5_at  \\\n",
       "17                6.573429            12.562943            12.435309   \n",
       "25                6.085707            12.580625            12.557963   \n",
       "14                6.483972            12.994929            13.036519   \n",
       "13                6.824745            13.123380            13.057701   \n",
       "48                6.209274            13.256962            13.003892   \n",
       "8                 6.794769            13.524380            13.400021   \n",
       "9                 6.913332            12.511607            12.470305   \n",
       "31                6.919148            12.889525            12.586034   \n",
       "32                6.646477            12.707219            12.559549   \n",
       "4                 6.700453            12.949352            12.782515   \n",
       "\n",
       "    AFFX-ThrX-3_at  AFFX-ThrX-5_at  AFFX-ThrX-M_at  AFFX-TrpnX-3_at  \\\n",
       "17        5.339725        4.535060        3.864028         3.576569   \n",
       "25        5.212672        4.553464        3.855921         3.655604   \n",
       "14        5.231631        4.602278        3.814854         3.581013   \n",
       "13        5.230581        4.416719        3.890692         3.551122   \n",
       "48        5.290075        4.514481        3.946614         3.602132   \n",
       "8         5.327728        4.631000        3.897626         3.671963   \n",
       "9         5.224274        4.517029        3.844456         3.587143   \n",
       "31        5.577722        4.555417        3.939002         3.558659   \n",
       "32        5.439620        4.661590        3.933793         3.686490   \n",
       "4         5.341689        4.560315        3.887020         3.629853   \n",
       "\n",
       "    AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  \n",
       "17         4.081583         3.985984  \n",
       "25         4.139994         4.046524  \n",
       "14         4.080418         4.073512  \n",
       "13         4.097900         3.976198  \n",
       "48         4.206121         4.103831  \n",
       "8          4.002604         4.058350  \n",
       "9          4.119111         3.985250  \n",
       "31         4.133050         4.331262  \n",
       "32         4.116434         4.041620  \n",
       "4          4.127513         4.004316  \n",
       "\n",
       "[10 rows x 22284 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df.iloc[:, 0] = label_encoder.fit_transform(df.iloc[:, 0].values)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>1007_s_at</th>\n",
       "      <th>1053_at</th>\n",
       "      <th>117_at</th>\n",
       "      <th>121_at</th>\n",
       "      <th>1255_g_at</th>\n",
       "      <th>1294_at</th>\n",
       "      <th>1316_at</th>\n",
       "      <th>1320_at</th>\n",
       "      <th>1405_i_at</th>\n",
       "      <th>...</th>\n",
       "      <th>AFFX-r2-Hs28SrRNA-5_at</th>\n",
       "      <th>AFFX-r2-Hs28SrRNA-M_at</th>\n",
       "      <th>AFFX-r2-P1-cre-3_at</th>\n",
       "      <th>AFFX-r2-P1-cre-5_at</th>\n",
       "      <th>AFFX-ThrX-3_at</th>\n",
       "      <th>AFFX-ThrX-5_at</th>\n",
       "      <th>AFFX-ThrX-M_at</th>\n",
       "      <th>AFFX-TrpnX-3_at</th>\n",
       "      <th>AFFX-TrpnX-5_at</th>\n",
       "      <th>AFFX-TrpnX-M_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2.965960</td>\n",
       "      <td>2.764308</td>\n",
       "      <td>2.717316</td>\n",
       "      <td>3.154539</td>\n",
       "      <td>2.172126</td>\n",
       "      <td>2.980647</td>\n",
       "      <td>2.467883</td>\n",
       "      <td>2.188620</td>\n",
       "      <td>3.104610</td>\n",
       "      <td>...</td>\n",
       "      <td>2.208132</td>\n",
       "      <td>2.716646</td>\n",
       "      <td>3.651103</td>\n",
       "      <td>3.636370</td>\n",
       "      <td>2.416765</td>\n",
       "      <td>2.181122</td>\n",
       "      <td>1.950105</td>\n",
       "      <td>1.838576</td>\n",
       "      <td>2.029129</td>\n",
       "      <td>1.994936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>2.937823</td>\n",
       "      <td>2.878198</td>\n",
       "      <td>2.927439</td>\n",
       "      <td>3.120076</td>\n",
       "      <td>2.133075</td>\n",
       "      <td>2.937996</td>\n",
       "      <td>2.473971</td>\n",
       "      <td>2.219950</td>\n",
       "      <td>2.441142</td>\n",
       "      <td>...</td>\n",
       "      <td>2.212119</td>\n",
       "      <td>2.605425</td>\n",
       "      <td>3.653132</td>\n",
       "      <td>3.650531</td>\n",
       "      <td>2.382023</td>\n",
       "      <td>2.186965</td>\n",
       "      <td>1.947075</td>\n",
       "      <td>1.870110</td>\n",
       "      <td>2.049629</td>\n",
       "      <td>2.016683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2.980555</td>\n",
       "      <td>2.709030</td>\n",
       "      <td>2.738417</td>\n",
       "      <td>3.151857</td>\n",
       "      <td>2.124297</td>\n",
       "      <td>2.995057</td>\n",
       "      <td>2.452341</td>\n",
       "      <td>2.258762</td>\n",
       "      <td>3.068596</td>\n",
       "      <td>...</td>\n",
       "      <td>2.286962</td>\n",
       "      <td>2.696878</td>\n",
       "      <td>3.699877</td>\n",
       "      <td>3.704487</td>\n",
       "      <td>2.387261</td>\n",
       "      <td>2.202348</td>\n",
       "      <td>1.931628</td>\n",
       "      <td>1.840368</td>\n",
       "      <td>2.028717</td>\n",
       "      <td>2.026273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2.948660</td>\n",
       "      <td>2.718879</td>\n",
       "      <td>2.756302</td>\n",
       "      <td>3.144894</td>\n",
       "      <td>2.162800</td>\n",
       "      <td>3.011515</td>\n",
       "      <td>2.469793</td>\n",
       "      <td>2.182010</td>\n",
       "      <td>2.853392</td>\n",
       "      <td>...</td>\n",
       "      <td>2.376025</td>\n",
       "      <td>2.770775</td>\n",
       "      <td>3.714067</td>\n",
       "      <td>3.706829</td>\n",
       "      <td>2.386971</td>\n",
       "      <td>2.142975</td>\n",
       "      <td>1.960027</td>\n",
       "      <td>1.828275</td>\n",
       "      <td>2.034885</td>\n",
       "      <td>1.991390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>3.018768</td>\n",
       "      <td>2.635271</td>\n",
       "      <td>3.206250</td>\n",
       "      <td>3.164874</td>\n",
       "      <td>2.190939</td>\n",
       "      <td>2.949110</td>\n",
       "      <td>2.501101</td>\n",
       "      <td>2.272872</td>\n",
       "      <td>2.952825</td>\n",
       "      <td>...</td>\n",
       "      <td>2.108990</td>\n",
       "      <td>2.634425</td>\n",
       "      <td>3.728678</td>\n",
       "      <td>3.700872</td>\n",
       "      <td>2.403288</td>\n",
       "      <td>2.174560</td>\n",
       "      <td>1.980615</td>\n",
       "      <td>1.848851</td>\n",
       "      <td>2.072490</td>\n",
       "      <td>2.036971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2.993866</td>\n",
       "      <td>2.761550</td>\n",
       "      <td>2.790978</td>\n",
       "      <td>3.166533</td>\n",
       "      <td>2.162864</td>\n",
       "      <td>2.941676</td>\n",
       "      <td>2.488121</td>\n",
       "      <td>2.270287</td>\n",
       "      <td>2.907345</td>\n",
       "      <td>...</td>\n",
       "      <td>2.245518</td>\n",
       "      <td>2.764424</td>\n",
       "      <td>3.757491</td>\n",
       "      <td>3.744163</td>\n",
       "      <td>2.413520</td>\n",
       "      <td>2.211324</td>\n",
       "      <td>1.962596</td>\n",
       "      <td>1.876552</td>\n",
       "      <td>2.000939</td>\n",
       "      <td>2.020893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2.943448</td>\n",
       "      <td>2.760252</td>\n",
       "      <td>2.726171</td>\n",
       "      <td>3.122046</td>\n",
       "      <td>2.151784</td>\n",
       "      <td>2.943384</td>\n",
       "      <td>2.459818</td>\n",
       "      <td>2.188405</td>\n",
       "      <td>3.319824</td>\n",
       "      <td>...</td>\n",
       "      <td>2.345501</td>\n",
       "      <td>2.789381</td>\n",
       "      <td>3.645195</td>\n",
       "      <td>3.640425</td>\n",
       "      <td>2.385230</td>\n",
       "      <td>2.175374</td>\n",
       "      <td>1.942780</td>\n",
       "      <td>1.842835</td>\n",
       "      <td>2.042333</td>\n",
       "      <td>1.994670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>2.969579</td>\n",
       "      <td>2.688057</td>\n",
       "      <td>2.683063</td>\n",
       "      <td>3.148375</td>\n",
       "      <td>2.214906</td>\n",
       "      <td>3.030613</td>\n",
       "      <td>2.490496</td>\n",
       "      <td>2.221689</td>\n",
       "      <td>2.579248</td>\n",
       "      <td>...</td>\n",
       "      <td>2.295317</td>\n",
       "      <td>2.790594</td>\n",
       "      <td>3.688127</td>\n",
       "      <td>3.653752</td>\n",
       "      <td>2.479676</td>\n",
       "      <td>2.187583</td>\n",
       "      <td>1.977830</td>\n",
       "      <td>1.831334</td>\n",
       "      <td>2.047207</td>\n",
       "      <td>2.114787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>2.957103</td>\n",
       "      <td>2.714081</td>\n",
       "      <td>2.751290</td>\n",
       "      <td>3.147987</td>\n",
       "      <td>2.191780</td>\n",
       "      <td>3.058226</td>\n",
       "      <td>2.459518</td>\n",
       "      <td>2.224815</td>\n",
       "      <td>2.533764</td>\n",
       "      <td>...</td>\n",
       "      <td>2.321983</td>\n",
       "      <td>2.732590</td>\n",
       "      <td>3.667576</td>\n",
       "      <td>3.650713</td>\n",
       "      <td>2.443506</td>\n",
       "      <td>2.220822</td>\n",
       "      <td>1.975921</td>\n",
       "      <td>1.882248</td>\n",
       "      <td>2.041395</td>\n",
       "      <td>2.014934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3.001823</td>\n",
       "      <td>2.888555</td>\n",
       "      <td>2.734666</td>\n",
       "      <td>3.178026</td>\n",
       "      <td>2.174721</td>\n",
       "      <td>3.066442</td>\n",
       "      <td>2.457782</td>\n",
       "      <td>2.281180</td>\n",
       "      <td>2.390956</td>\n",
       "      <td>...</td>\n",
       "      <td>2.407405</td>\n",
       "      <td>2.744259</td>\n",
       "      <td>3.694808</td>\n",
       "      <td>3.676100</td>\n",
       "      <td>2.417296</td>\n",
       "      <td>2.189133</td>\n",
       "      <td>1.958665</td>\n",
       "      <td>1.859911</td>\n",
       "      <td>2.045273</td>\n",
       "      <td>2.001556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  1007_s_at   1053_at    117_at    121_at  1255_g_at   1294_at  \\\n",
       "17    1   2.965960  2.764308  2.717316  3.154539   2.172126  2.980647   \n",
       "25    0   2.937823  2.878198  2.927439  3.120076   2.133075  2.937996   \n",
       "14    1   2.980555  2.709030  2.738417  3.151857   2.124297  2.995057   \n",
       "13    1   2.948660  2.718879  2.756302  3.144894   2.162800  3.011515   \n",
       "48    3   3.018768  2.635271  3.206250  3.164874   2.190939  2.949110   \n",
       "8     1   2.993866  2.761550  2.790978  3.166533   2.162864  2.941676   \n",
       "9     1   2.943448  2.760252  2.726171  3.122046   2.151784  2.943384   \n",
       "31    0   2.969579  2.688057  2.683063  3.148375   2.214906  3.030613   \n",
       "32    0   2.957103  2.714081  2.751290  3.147987   2.191780  3.058226   \n",
       "4     2   3.001823  2.888555  2.734666  3.178026   2.174721  3.066442   \n",
       "\n",
       "     1316_at   1320_at  1405_i_at  ...  AFFX-r2-Hs28SrRNA-5_at  \\\n",
       "17  2.467883  2.188620   3.104610  ...                2.208132   \n",
       "25  2.473971  2.219950   2.441142  ...                2.212119   \n",
       "14  2.452341  2.258762   3.068596  ...                2.286962   \n",
       "13  2.469793  2.182010   2.853392  ...                2.376025   \n",
       "48  2.501101  2.272872   2.952825  ...                2.108990   \n",
       "8   2.488121  2.270287   2.907345  ...                2.245518   \n",
       "9   2.459818  2.188405   3.319824  ...                2.345501   \n",
       "31  2.490496  2.221689   2.579248  ...                2.295317   \n",
       "32  2.459518  2.224815   2.533764  ...                2.321983   \n",
       "4   2.457782  2.281180   2.390956  ...                2.407405   \n",
       "\n",
       "    AFFX-r2-Hs28SrRNA-M_at  AFFX-r2-P1-cre-3_at  AFFX-r2-P1-cre-5_at  \\\n",
       "17                2.716646             3.651103             3.636370   \n",
       "25                2.605425             3.653132             3.650531   \n",
       "14                2.696878             3.699877             3.704487   \n",
       "13                2.770775             3.714067             3.706829   \n",
       "48                2.634425             3.728678             3.700872   \n",
       "8                 2.764424             3.757491             3.744163   \n",
       "9                 2.789381             3.645195             3.640425   \n",
       "31                2.790594             3.688127             3.653752   \n",
       "32                2.732590             3.667576             3.650713   \n",
       "4                 2.744259             3.694808             3.676100   \n",
       "\n",
       "    AFFX-ThrX-3_at  AFFX-ThrX-5_at  AFFX-ThrX-M_at  AFFX-TrpnX-3_at  \\\n",
       "17        2.416765        2.181122        1.950105         1.838576   \n",
       "25        2.382023        2.186965        1.947075         1.870110   \n",
       "14        2.387261        2.202348        1.931628         1.840368   \n",
       "13        2.386971        2.142975        1.960027         1.828275   \n",
       "48        2.403288        2.174560        1.980615         1.848851   \n",
       "8         2.413520        2.211324        1.962596         1.876552   \n",
       "9         2.385230        2.175374        1.942780         1.842835   \n",
       "31        2.479676        2.187583        1.977830         1.831334   \n",
       "32        2.443506        2.220822        1.975921         1.882248   \n",
       "4         2.417296        2.189133        1.958665         1.859911   \n",
       "\n",
       "    AFFX-TrpnX-5_at  AFFX-TrpnX-M_at  \n",
       "17         2.029129         1.994936  \n",
       "25         2.049629         2.016683  \n",
       "14         2.028717         2.026273  \n",
       "13         2.034885         1.991390  \n",
       "48         2.072490         2.036971  \n",
       "8          2.000939         2.020893  \n",
       "9          2.042333         1.994670  \n",
       "31         2.047207         2.114787  \n",
       "32         2.041395         2.014934  \n",
       "4          2.045273         2.001556  \n",
       "\n",
       "[10 rows x 22284 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 1:] = np.log2(df.iloc[:, 1:].values)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17    1\n",
       "25    0\n",
       "14    1\n",
       "13    1\n",
       "48    3\n",
       "     ..\n",
       "36    0\n",
       "29    0\n",
       "62    4\n",
       "61    4\n",
       "53    3\n",
       "Name: type, Length: 64, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, 1:]\n",
    "y = df['type']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype('int')\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "X_train = pd.DataFrame(sc.fit_transform(X_train.values), columns=X_train.columns)\n",
    "X_test = sc.fit_transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm_pred = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 0 0 0 0]\n",
      " [1 3 0 0 0]\n",
      " [4 0 0 0 0]\n",
      " [0 0 0 2 0]\n",
      " [1 0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.72727272727273"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy= accuracy_score(y_test, svm_pred)\n",
    "test_accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracy = accuracy_score(y_train, svm_model.predict(X_train))\n",
    "train_accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272727272727273"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, svm_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.73         8\n",
      "           1       1.00      0.75      0.86         4\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.73        22\n",
      "   macro avg       0.71      0.70      0.69        22\n",
      "weighted avg       0.66      0.73      0.67        22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lg_model = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lg_predict = lg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_train_accuracy = accuracy_score(y_train, lg_model.predict(X_train))\n",
    "lg_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_test_accuracy = accuracy_score(y_test, lg_predict)\n",
    "lg_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         4\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00        22\n",
      "   macro avg       1.00      1.00      1.00        22\n",
      "weighted avg       1.00      1.00      1.00        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lg_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = LinearDiscriminantAnalysis().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ld_predict = ld.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9545454545454546"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld_test_accuracy_score = accuracy_score(y_test, ld_predict)\n",
    "ld_test_accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.09523809523809"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld_train_accuracy_score = accuracy_score(y_train, ld.predict(X_train))\n",
    "ld_train_accuracy_score * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94         8\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         4\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.95        22\n",
      "   macro avg       0.98      0.95      0.96        22\n",
      "weighted avg       0.96      0.95      0.95        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, ld_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rf_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_train_accuracy = accuracy_score(y_train, rf.predict(X_train))\n",
    "rf_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.9090909090909"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_test_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_test_accuracy * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nb_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  50.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_test, nb_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_train_accuracy = accuracy_score(y_train, nb.predict(X_train))\n",
    "nb_train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lgreg = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\leukemi_microarray\\.venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lgreg_predict = lgreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_test_accuracy = accuracy_score(y_test, lgreg_predict)\n",
    "log_reg_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_train_accuracy = accuracy_score(y_train, lgreg.predict(X_train))\n",
    "log_reg_train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       1.00      1.00      1.00         4\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00        22\n",
      "   macro avg       1.00      1.00      1.00        22\n",
      "weighted avg       1.00      1.00      1.00        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lgreg_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 0 0 0 0]\n",
      " [0 4 0 0 0]\n",
      " [0 0 4 0 0]\n",
      " [0 0 0 2 0]\n",
      " [0 0 0 0 4]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, lgreg_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
